{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1\t\t\n",
    " Create an embedding table for relative positions\n",
    "##### Step 2\t\n",
    " Compute relative distances between query and key positions\n",
    "##### Step 3\t\n",
    "Clip distances to be within [-max_relative_position, max_relative_position]\n",
    "##### Step 4\t\n",
    "Convert to indices for lookup in embedding table\n",
    "##### Step5\t\n",
    "Retrieve embeddings for each relative position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `num_units:` Embedding size (e.g., `num_units=8` means each relative position is encoded as an 8-dimensional vector).\n",
    "- `max_relative_position:` Maximum relative distance (e.g., `max_relative_position=4 `means relative positions range from -4 to +4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units = 8\n",
    "max_relative_position = 2\n",
    "length_q = 3\n",
    "length_k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `embeddings_table:` A trainable matrix of shape `(2 * max_relative_position + 1, num_units)`.\n",
    "\n",
    "- Each row represents a `relative position from -max_relative_position` to +max_relative_position.\n",
    "- Example: If` max_relative_position=4` and `num_units=8`, the shape of `embeddings_table` will be (9, 8) because positions range from` -4 to +4`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[4.2039e-45, 7.0065e-45, 4.9739e+22, 8.4218e-43, 5.9271e+22, 8.4218e-43,\n",
       "         8.5149e+22, 8.4218e-43],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4013e-45, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [2.1019e-44, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00]], requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_table=nn.Parameter(torch.Tensor(max_relative_position*2+1,num_units))\n",
    "embeddings_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `nn.init.xavier_uniform_():` This initializes the embedding table with values that help with stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1740, -0.1894,  0.1764,  0.4095, -0.6246,  0.4744, -0.2800,  0.2685],\n",
       "        [-0.1422, -0.5910,  0.1839, -0.2461,  0.1772, -0.2254, -0.6317, -0.3293],\n",
       "        [ 0.2137,  0.5104,  0.2616, -0.5953,  0.2767, -0.1704,  0.1310, -0.2576],\n",
       "        [-0.7897, -1.3042, -0.2933,  0.1419,  0.5016, -0.4019,  0.9015,  0.0623],\n",
       "        [-0.3086, -0.3942,  0.3056,  0.0655, -0.4996, -0.2155, -0.0900,  0.7124]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.xavier_normal_(embeddings_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- range_vec_q creates a tensor [0, 1, 2, ..., length_q-1].\n",
    "- range_vec_k creates a tensor [0, 1, 2, ..., length_k-1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "range_vec_q=torch.arange(length_q)\n",
    "range_vec_k=torch.arange(length_k)\n",
    "print(range_vec_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Computes a **distance matrix** where each element is the difference between query and key positions.\n",
    "- Example for `length_q=3`, `length_k=3`:\n",
    "\n",
    "The distance between position `i` (query) and `j (key)` is `j - i.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=range_vec_k[None,:]\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j=range_vec_q[:,None]\n",
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, -1, -2],\n",
       "        [ 1,  0, -1],\n",
       "        [ 2,  1,  0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_mat=j-i\n",
    "distance_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Clips` the distance values so that they stay within the range `[-max_relative_position, max_relative_position].`\n",
    "- Example with `max_relative_position=2`:\n",
    "\n",
    "- If a distance exceeds `max_relative_position`, it is **clamped** to `Â±max_relative_position.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, -1, -2],\n",
       "        [ 1,  0, -1],\n",
       "        [ 2,  1,  0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_mat_clipped=torch.clamp(distance_mat,-max_relative_position,max_relative_position)\n",
    "distance_mat_clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Shifts all values to be positive** so they can be used as indices in the embedding table.\n",
    "- Example `(with max_relative_position=2)`:\n",
    "\n",
    "- This ensures the smallest relative position `(-2)` maps to index `0`, and the largest `(+2)` maps to index `4`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 0],\n",
       "        [3, 2, 1],\n",
       "        [4, 3, 2]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_mat=distance_mat_clipped+max_relative_position\n",
    "final_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uses `final_mat` as indices to select the corresponding embeddings from `embeddings_table.`\n",
    "- The shape of `embeddings` will be (length_q, length_k, num_units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2137,  0.5104,  0.2616, -0.5953,  0.2767, -0.1704,  0.1310,\n",
       "          -0.2576],\n",
       "         [-0.1422, -0.5910,  0.1839, -0.2461,  0.1772, -0.2254, -0.6317,\n",
       "          -0.3293],\n",
       "         [ 0.1740, -0.1894,  0.1764,  0.4095, -0.6246,  0.4744, -0.2800,\n",
       "           0.2685]],\n",
       "\n",
       "        [[-0.7897, -1.3042, -0.2933,  0.1419,  0.5016, -0.4019,  0.9015,\n",
       "           0.0623],\n",
       "         [ 0.2137,  0.5104,  0.2616, -0.5953,  0.2767, -0.1704,  0.1310,\n",
       "          -0.2576],\n",
       "         [-0.1422, -0.5910,  0.1839, -0.2461,  0.1772, -0.2254, -0.6317,\n",
       "          -0.3293]],\n",
       "\n",
       "        [[-0.3086, -0.3942,  0.3056,  0.0655, -0.4996, -0.2155, -0.0900,\n",
       "           0.7124],\n",
       "         [-0.7897, -1.3042, -0.2933,  0.1419,  0.5016, -0.4019,  0.9015,\n",
       "           0.0623],\n",
       "         [ 0.2137,  0.5104,  0.2616, -0.5953,  0.2767, -0.1704,  0.1310,\n",
       "          -0.2576]]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_table[final_mat]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
