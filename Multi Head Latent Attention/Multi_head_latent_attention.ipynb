{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d41bf3",
   "metadata": {},
   "source": [
    "## Step 1: Writting the code for the Multi Head Latent Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b107d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f99b2047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RopelessMLA(nn.Module):\n",
    "    def __init__(self,d_model,n_heads,kv_latent_dim):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.n_heads=n_heads\n",
    "        self.dh=d_model//n_heads ## Dimension per head\n",
    "\n",
    "        ## Projection layers\n",
    "        self.W_q=nn.Linear(d_model,d_model,bias=False) ## Query Projection\n",
    "        self.W_dkv=nn.Linear(d_model,kv_latent_dim,bias=False) ## Compress into latent kv space\n",
    "        self.W_uk=nn.Linear(kv_latent_dim,d_model,bias=False) ## Decompress K\n",
    "        self.W_uv=nn.Linear(kv_latent_dim,d_model,bias=False) ## Decompress V\n",
    "        self.W_o=nn.Linear(d_model,d_model,bias=False)  ## Final Output projection\n",
    "        \n",
    "\n",
    "        self.ln=nn.LayerNorm(kv_latent_dim)\n",
    "        self.register_buffer(\"absorbed_k\",None) ### Holds w_q @ W_uk\n",
    "\n",
    "    def forward(self,x,kv_cache=None,past_length=0):\n",
    "            B,S,D=x.size()\n",
    "            ## Compute absorbed_k once : W_q @ W_uk ,shape: (D,latent_dim)\n",
    "            if self.absorbed_k is None :\n",
    "                absorbed=torch.matmul(self.W_q.weight,self.W_uk.weight) ## (D,latent_dim)\n",
    "                self.absorbed_k=absorbed.view(self.n_heads,self.dh,-1) ## (n_heads,dh,latent_dim)\n",
    "            \n",
    "            ## Compress x into latent KV Space\n",
    "            new_c_kv=self.ln(self.W_dkv(x)) ## (B,S,latent_dim)\n",
    "            if kv_cache is None:\n",
    "                c_kv=new_c_kv\n",
    "            else:\n",
    "                c_kv=torch.cat((kv_cache,new_c_kv),dim=1) ## (B_total,latent_dim)\n",
    "            S_full=c_kv.size(1)\n",
    "            \n",
    "            ## Decompress V to full d_model and split into Heads\n",
    "            v_full=self.W_uv(c_kv)  ## (B,S_full,D)\n",
    "            v=v_full.view(B,S,self.n_heads,self.dh) ## (B,S,n_heads,dh)\n",
    "\n",
    "            ## Use input  x directly (Since W_q is absorbed)\n",
    "            q=x.view(B,S,self.n_heads,self.dh) ## (B,S,n_heads,dh)\n",
    "            \n",
    "            ## Compute attention scores\n",
    "            attn_scores=torch.zeros(B,self.n_heads,S,S_full)\n",
    "            for h in range(self.n_heads):\n",
    "                tmp=torch.matmul(q[:,:,h],self.absorbed_k[h])\n",
    "                attn_scores[:,h]=torch.bmm(tmp,c_kv.transpose(1,2))\n",
    "\n",
    "            ## Scale and Apply Causal Mask\n",
    "\n",
    "            attn_scores=attn_scores/(self.dh*0.5)\n",
    "            mask=torch.tril(torch.ones((S,S_full),device=x.device),diagonal=past_length)\n",
    "            attn_scores=attn_scores.masked_fill(mask.view(1,1,S,S_full)==0,float('-inf'))\n",
    "\n",
    "            ## Apply Softmax to get attention weights\n",
    "            attn_weights=torch.softmax(attn_scores,dim=-1) ## (B,n_heads,S,S_full)\n",
    "\n",
    "            ## Apply attention weights to each heads V separately\n",
    "            out_heads=[]\n",
    "            for h in range(self.n_heads):\n",
    "                context_h=torch.bmm(attn_weights[:,h],v[:,:,h])  ## (B,S,dh)\n",
    "                out_heads.append(context_h)\n",
    "            ## Concentenate all head outputs along the feature dimension\n",
    "            out=torch.cat(out_heads,dim=-1)  ## (B,S,D)\n",
    "            return self.W_o(out),c_kv ## Final output projection updated latent cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c80430bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model=RopelessMLA(d_model=512,n_heads=8,kv_latent_dim=256)\n",
    "    x=torch.randn(1,5,512) ## Batch=2,context_length=10,d_model=512\n",
    "    out,cache=model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4bb479",
   "metadata": {},
   "source": [
    "## Step 2 Memory Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "631c9d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([1, 5, 512]), Cache: torch.Size([1, 5, 256])\n",
      "Memory: Standard=80.0kb ,Latent=20.0KB,Reduction=4.0\n"
     ]
    }
   ],
   "source": [
    "def demo ():\n",
    "    model=RopelessMLA(d_model=512,n_heads=8,kv_latent_dim=256)\n",
    "    x=torch.randn(1,5,512) ## Batch=2,context_length=10,d_model=512\n",
    "    out,cache=model(x)\n",
    "    print(f\"Output: {out.shape}, Cache: {cache.shape}\")\n",
    "\n",
    "    ## Memory Consumption\n",
    "    std_size=2*2*10*512*4/1024\n",
    "    latent_size=2*10*256*4/1024\n",
    "    print(f\"Memory: Standard={std_size:.1f}kb ,Latent={latent_size:.1f}KB,Reduction={std_size/latent_size}\")\n",
    "\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c9e0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
