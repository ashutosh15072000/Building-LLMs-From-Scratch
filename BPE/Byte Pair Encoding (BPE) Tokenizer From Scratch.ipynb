{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa154995",
   "metadata": {},
   "source": [
    "## 1. The main idea behind byte pair encoding (BPE)\n",
    "\n",
    "- The main idea in BPE is to convert text into an integer representation (token IDs) for LLM training\n",
    "\n",
    "![alt text](IMG/tokenization.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93ec7f",
   "metadata": {},
   "source": [
    "## 1.1 Bits and Bytes\n",
    "\n",
    "- Before getting to the BPE algorithm, let's introduce the notion of bytes.\n",
    "- Consider converting text into a btye array (BPE stands for \"btye\" pair encoding after all):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2050b259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'This is some text')\n"
     ]
    }
   ],
   "source": [
    "text=\"This is some text\"\n",
    "byte_arry=bytearray(text,\"utf-8\")\n",
    "print(byte_arry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696ba5b",
   "metadata": {},
   "source": [
    "- When we call `list()` on a `bytearray` object,each byte is treated as an individual elements and the result is a list of integers corresponding to the byte values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194af6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 104, 105, 115, 32, 105, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116]\n"
     ]
    }
   ],
   "source": [
    "ids = list(byte_arry)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369004b9",
   "metadata": {},
   "source": [
    "- This would be a valid way to correct text into a token ID representation that we need for the embedding layer of an LLM.\n",
    "\n",
    "- However,the downside of this approach is that is creating one ID for each character(that's a lot of IDs for a short text!)\n",
    "\n",
    "- this means for a 17 character input text,we have to use 17 token IDs as input to the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ffa89de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 17\n",
      "Number of token IDs: 17\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(text))\n",
    "print(\"Number of token IDs:\", len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c0d0a1",
   "metadata": {},
   "source": [
    "- If you have worked with LLMs before you may know that BPE tokenizers have a vocabulary where we have a token IDs for whole words or subwords instead of each character.\n",
    "\n",
    "- For example , the GPT-2 tokenizer tokenizes the small text(\"This is some text\") into only instead of 17 tokens: `1212, 318,617,2420`\n",
    "\n",
    "- You can double-check this using the interactive [tiktoken app](https://tiktokenizer.vercel.app/?model=gpt2) or the [tiktoken library:](https://github.com/openai/tiktoken)\n",
    "\n",
    "![alt text](IMG/tiktokenizer.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f0138e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212, 318, 617, 2420]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "gpt2_tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2_tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb554b66",
   "metadata": {},
   "source": [
    "- Since a byte consists of 8 bits, there are 28 = 256 possible values that a single byte can represent, ranging from 0 to 255\n",
    "\n",
    "- You can confirm this by executing the code `bytearray(range(0, 257))`, which will warn you that` ValueError: byte must be in range(0, 256))`\n",
    "\n",
    "- A BPE tokenizer usually uses these 256 values as its first 256 single-character tokens; one could visually check this by running the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ea05497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: !\n",
      "1: \"\n",
      "2: #\n",
      "3: $\n",
      "4: %\n",
      "5: &\n",
      "6: '\n",
      "7: (\n",
      "8: )\n",
      "9: *\n",
      "10: +\n",
      "11: ,\n",
      "12: -\n",
      "13: .\n",
      "14: /\n",
      "15: 0\n",
      "16: 1\n",
      "17: 2\n",
      "18: 3\n",
      "19: 4\n",
      "20: 5\n",
      "21: 6\n",
      "22: 7\n",
      "23: 8\n",
      "24: 9\n",
      "25: :\n",
      "26: ;\n",
      "27: <\n",
      "28: =\n",
      "29: >\n",
      "30: ?\n",
      "31: @\n",
      "32: A\n",
      "33: B\n",
      "34: C\n",
      "35: D\n",
      "36: E\n",
      "37: F\n",
      "38: G\n",
      "39: H\n",
      "40: I\n",
      "41: J\n",
      "42: K\n",
      "43: L\n",
      "44: M\n",
      "45: N\n",
      "46: O\n",
      "47: P\n",
      "48: Q\n",
      "49: R\n",
      "50: S\n",
      "51: T\n",
      "52: U\n",
      "53: V\n",
      "54: W\n",
      "55: X\n",
      "56: Y\n",
      "57: Z\n",
      "58: [\n",
      "59: \\\n",
      "60: ]\n",
      "61: ^\n",
      "62: _\n",
      "63: `\n",
      "64: a\n",
      "65: b\n",
      "66: c\n",
      "67: d\n",
      "68: e\n",
      "69: f\n",
      "70: g\n",
      "71: h\n",
      "72: i\n",
      "73: j\n",
      "74: k\n",
      "75: l\n",
      "76: m\n",
      "77: n\n",
      "78: o\n",
      "79: p\n",
      "80: q\n",
      "81: r\n",
      "82: s\n",
      "83: t\n",
      "84: u\n",
      "85: v\n",
      "86: w\n",
      "87: x\n",
      "88: y\n",
      "89: z\n",
      "90: {\n",
      "91: |\n",
      "92: }\n",
      "93: ~\n",
      "94: �\n",
      "95: �\n",
      "96: �\n",
      "97: �\n",
      "98: �\n",
      "99: �\n",
      "100: �\n",
      "101: �\n",
      "102: �\n",
      "103: �\n",
      "104: �\n",
      "105: �\n",
      "106: �\n",
      "107: �\n",
      "108: �\n",
      "109: �\n",
      "110: �\n",
      "111: �\n",
      "112: �\n",
      "113: �\n",
      "114: �\n",
      "115: �\n",
      "116: �\n",
      "117: �\n",
      "118: �\n",
      "119: �\n",
      "120: �\n",
      "121: �\n",
      "122: �\n",
      "123: �\n",
      "124: �\n",
      "125: �\n",
      "126: �\n",
      "127: �\n",
      "128: �\n",
      "129: �\n",
      "130: �\n",
      "131: �\n",
      "132: �\n",
      "133: �\n",
      "134: �\n",
      "135: �\n",
      "136: �\n",
      "137: �\n",
      "138: �\n",
      "139: �\n",
      "140: �\n",
      "141: �\n",
      "142: �\n",
      "143: �\n",
      "144: �\n",
      "145: �\n",
      "146: �\n",
      "147: �\n",
      "148: �\n",
      "149: �\n",
      "150: �\n",
      "151: �\n",
      "152: �\n",
      "153: �\n",
      "154: �\n",
      "155: �\n",
      "156: �\n",
      "157: �\n",
      "158: �\n",
      "159: �\n",
      "160: �\n",
      "161: �\n",
      "162: �\n",
      "163: �\n",
      "164: �\n",
      "165: �\n",
      "166: �\n",
      "167: �\n",
      "168: �\n",
      "169: �\n",
      "170: �\n",
      "171: �\n",
      "172: �\n",
      "173: �\n",
      "174: �\n",
      "175: �\n",
      "176: �\n",
      "177: �\n",
      "178: �\n",
      "179: �\n",
      "180: �\n",
      "181: �\n",
      "182: �\n",
      "183: �\n",
      "184: �\n",
      "185: �\n",
      "186: �\n",
      "187: �\n",
      "188: \u0000\n",
      "189: \u0001\n",
      "190: \u0002\n",
      "191: \u0003\n",
      "192: \u0004\n",
      "193: \u0005\n",
      "194: \u0006\n",
      "195: \u0007\n",
      "196:\n",
      "197: \t\n",
      "198: \n",
      "\n",
      "199: \u000b\n",
      "200: \f\n",
      "201: \n",
      "202: \u000e\n",
      "203: \u000f\n",
      "204: \u0010\n",
      "205: \u0011\n",
      "206: \u0012\n",
      "207: \u0013\n",
      "208: \u0014\n",
      "209: \u0015\n",
      "210: \u0016\n",
      "211: \u0017\n",
      "212: \u0018\n",
      "213: \u0019\n",
      "214: \u001a\n",
      "215: \u001b\n",
      "216: \u001c\n",
      "217: \u001d\n",
      "218: \u001e\n",
      "219: \u001f\n",
      "220:  \n",
      "221: \n",
      "222: �\n",
      "223: �\n",
      "224: �\n",
      "225: �\n",
      "226: �\n",
      "227: �\n",
      "228: �\n",
      "229: �\n",
      "230: �\n",
      "231: �\n",
      "232: �\n",
      "233: �\n",
      "234: �\n",
      "235: �\n",
      "236: �\n",
      "237: �\n",
      "238: �\n",
      "239: �\n",
      "240: �\n",
      "241: �\n",
      "242: �\n",
      "243: �\n",
      "244: �\n",
      "245: �\n",
      "246: �\n",
      "247: �\n",
      "248: �\n",
      "249: �\n",
      "250: �\n",
      "251: �\n",
      "252: �\n",
      "253: �\n",
      "254: �\n",
      "255: �\n",
      "256:  t\n",
      "257:  a\n",
      "258: he\n",
      "259: in\n",
      "260: re\n",
      "261: on\n",
      "262:  the\n",
      "263: er\n",
      "264:  s\n",
      "265: at\n",
      "266:  w\n",
      "267:  o\n",
      "268: en\n",
      "269:  c\n",
      "270: it\n",
      "271: is\n",
      "272: an\n",
      "273: or\n",
      "274: es\n",
      "275:  b\n",
      "276: ed\n",
      "277:  f\n",
      "278: ing\n",
      "279:  p\n",
      "280: ou\n",
      "281:  an\n",
      "282: al\n",
      "283: ar\n",
      "284:  to\n",
      "285:  m\n",
      "286:  of\n",
      "287:  in\n",
      "288:  d\n",
      "289:  h\n",
      "290:  and\n",
      "291: ic\n",
      "292: as\n",
      "293: le\n",
      "294:  th\n",
      "295: ion\n",
      "296: om\n",
      "297: ll\n",
      "298: ent\n",
      "299:  n\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(300):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6898d6cb",
   "metadata": {},
   "source": [
    "- Above, note that entries 256 and 257 are not single-character values but double-character values (a whitespace + a letter), which is a little shortcoming of the original GPT-2 BPE Tokenizer (this has been improved in the GPT-4 tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda04fb",
   "metadata": {},
   "source": [
    "1.2 Building the vocabulary\n",
    "\n",
    "- The goal of the BPE tokenization algorthim is to build a vocabulary of commonly occuring subwords like `298:ent` (which can be founded in entangle,enertain,entrance,entity,......,for example), or even complete words like \n",
    "\n",
    "318: is\n",
    "617:some\n",
    "1212: This\n",
    "2420: text\n",
    "\n",
    "1.3 BPE algorithm outline\n",
    "\n",
    "**1. Identify frequent pairs**\n",
    "\n",
    "- In each iteration,scan the text to find the most commomly occuring pair of bytes(or characters)\n",
    "\n",
    "**2. Replace and record**\n",
    "\n",
    "- Replace that pair with a new placeholder ID(one not already in use ,e.g if we start with 0...255, the first placeholder would be 256).\n",
    "\n",
    "- Record this mapping in alookup table.\n",
    "- The size of the lookup table is hyperparameter also called \"vocab size(for GPT2 that's 50257)\"\n",
    "\n",
    "**3. Repeat until no gain**\n",
    "\n",
    "- Keep repeating steps 1 and 2, continually merging the most frequent pairs\n",
    "\n",
    "- Stop when no further compression is possible(e.g no pairs occurs more than once).\n",
    "\n",
    "**Decompression(decoding)**\n",
    "\n",
    "To restore the original text, reverse the process by substituting each ID with its corresponding pair, using the lookup table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d9d7c",
   "metadata": {},
   "source": [
    "## 1.4 BPE algorithm example\n",
    "\n",
    "**1.4.1 Concrete example of the encoding part(steps 1 & 2 in section 1.3)**\n",
    "\n",
    "- Suppose we have the text (training dataset) `the cat in the hat` from which we want to build the vocabulary for a BPE Tokenizer.\n",
    "\n",
    "**Iteration 1**\n",
    "\n",
    "`1. Identify the frequent pairs`\n",
    "\n",
    "- In this text \"th\" appears twice (at the begiining and before the second \"e\")\n",
    "\n",
    "`2. Replace and record`\n",
    "\n",
    "- Replace \"th\" with new token ID that is not already in use ,e.g 256.\n",
    "- The new text is: `<256>e cat in <256>e hat`\n",
    "- The new vocabulary is \n",
    "\n",
    "        0:...\n",
    "\n",
    "        ...\n",
    "\n",
    "        256: \"th\"\n",
    "\n",
    "**Iteration 2**\n",
    "\n",
    "`1. Identify frequent pairs`\n",
    "\n",
    "    - In the text `<256>e cat in <256>hat`, the pair `<256>e` appears twice \n",
    "`2. Replace and record`\n",
    "    - Replace `<256e>` with new token ID that is not already in use, for example, `257.`\n",
    "\n",
    "    -  The new text is:\n",
    "        <257> cat in <257> hat\n",
    "\n",
    "    - The updated vocabulary is:\n",
    "        0:..\n",
    "\n",
    "        ...\n",
    "\n",
    "        256: \"th\"\n",
    "\n",
    "        257: \"<256>e\"\n",
    "\n",
    "**Iteration 3**\n",
    "\n",
    "`1.Identify frequent pairs`\n",
    "\n",
    "- In the text `<257> cat in <257> hat`, the pair `<257>`  appears twice (once at the beginning and once before “hat”).\n",
    "\n",
    "`2.Replace and record`\n",
    "\n",
    "- Replace <257>  with a new token ID that is not already in use, for example, 258.\n",
    "- The new text is:\n",
    "<258>cat in <258>hat\n",
    "- The updated vocabulary is:\n",
    "0: ...\n",
    "...\n",
    "256: \"th\"\n",
    "257: \"<256>e\"\n",
    "258: \"<257> \"\n",
    "- and so forth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8044f5",
   "metadata": {},
   "source": [
    "## 1.4.2 Concrete example of the decoding part (step 3 in section 1.3)\n",
    "\n",
    "- To restore the original text, we reverse the process by substituting each token ID with its corresponding pair in the reverse order they were introduced\n",
    "- Start with the final compressed text: `<258>cat in <258>hat`\n",
    "- Substitute `<258> → <257> : <257> cat in <257> hat`\n",
    "- Substitute `<257> → <256>e: <256>e cat in <256>e hat`\n",
    "- Substitute `<256> → \"th\": the cat in the hat`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d0638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "import json\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        ## MAP TOKEN ID TO TOKEN_STR (e.g {11246:\"some\"})\n",
    "        self.vocab={}\n",
    "        ## MAP TOKEN_STR TO TOKEN_ID (e.g {\"some\": 11246})\n",
    "        self.inverse_vocab={}\n",
    "        ## DICTIONARY OF BPE MERGES: {(token_id1,token_id1): merged_token_id}\n",
    "        self.bpe_merges={}\n",
    "        ## FOR THE OFFICAL OPENAI GPT-2 MERGES , USES A RANK DICT:\n",
    "        ## OF FORM {(string_A,string_B): rank}, WHERE LOWER RANK= HIGER PRIORITY\n",
    "        self.bpe_ranks={}\n",
    "    \n",
    "    def train(self,text,vocab_size,allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        TRAIN THE BPE TOKENIZER FROM SCRATCH.\n",
    "\n",
    "        Args:\n",
    "            text (_type_): _description_\n",
    "            vocab_size (_type_): _description_\n",
    "            allowed_special (dict, optional): _description_. Defaults to {\"<|endoftext|>\"}.\n",
    "        \"\"\"\n",
    "\n",
    "        ## PREPROCESS: REPLACE SPACES with \"Ġ\"\n",
    "        ## NOTE THAT \"Ġ\" IS A PARTICULARY OF THE GPT-2 BPE IMPLEMENTATION\n",
    "        ## E.g \"Hello World\" MIGHT BE TOKENIZED AS [\"Hello\",\"Ġworld\"]\n",
    "        ## (GPT-4 BPE would tokenize it as ['Hello','world'])\n",
    "        processed_text=[]\n",
    "        for i,char in enumerate(text):\n",
    "            if char==\" \" and i!=0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char !=\" \":\n",
    "                processed_text.append(char)\n",
    "        \n",
    "        \n",
    "        ## INITIALIZE VOCAB WITH UNIQUE CHARACTERS INCLUDING   \"Ġ\" IF PRESENT\n",
    "        ## START WITH FIRST 256 ASCII CHARACTERS\n",
    "\n",
    "        unique_chars=[chr(i) for i in range(256)]\n",
    "        unique_chars.extend(\n",
    "            char for char in sorted(set(processed_text))\n",
    "            if char not in unique_chars\n",
    "            \n",
    "        )\n",
    "\n",
    "        if \"Ġ\" not in unique_chars:\n",
    "            unique_chars.append(\"Ġ\")\n",
    "        \n",
    "        self.vocab = {i: char for i,char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char : i for i,char in self.vocab.items()}\n",
    "\n",
    "        ## ADD ALLOWED SPECIAL TOKENS\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id=len(self.vocab)\n",
    "                    self.vocab[new_id]=token\n",
    "                    self.inverse_vocab[token]=new_id\n",
    "\n",
    "        \n",
    "        ## TOKENIZE THE PREPROCESSED TEXT INTO TOKEN IDs\n",
    "        token_ids=[self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        ## BPE STEP 1-3: Repeatedly find and replace frequent pairs\n",
    "        for new_ids in range(len(self.vocab),vocab_size):\n",
    "            pair_id=self.find_freq_pair(token_ids,mode=\"most\")\n",
    "            if pair_id==None:\n",
    "                break\n",
    "            token_ids=self.replace_pair(token_ids,pair_id,new_id)\n",
    "            self.bpe_merges[pair_id]=new_ids\n",
    "        ## BUILD THE VOCABULARY WITH MERGED TOKENS\n",
    "        for (p0,p1),new_id in self.bpe_merges.items():\n",
    "            mereged_token=self.vocab[new_id]=self.vocab[p0]+self.vocab[p1]\n",
    "            self.vocab[new_id]=mereged_token\n",
    "            self.inverse_vocab[mereged_token]=new_id\n",
    "    def encode(self,text,allowed_special=None):\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs, with tiktoken-style handling of special tokens.\n",
    "    \n",
    "        Args:\n",
    "            text (str): The input text to encode.\n",
    "            allowed_special (set or None): Special tokens to allow passthrough. If None, special handling is disabled.\n",
    "    \n",
    "        Returns:\n",
    "            List of token IDs.\n",
    "        \"\"\"\n",
    "        import re\n",
    "        token_ids=[]\n",
    "\n",
    "        ## IF SPECIAL TOKEN HANDLING IS ENABLED\n",
    "        if allowed_special is not None and len(allowed_special)>0:\n",
    "            ## BUILD REGEX TO MATCH ALLOWED SPECIAL TOKENS\n",
    "            special_pattern=(\n",
    "                \"(\"+\"|\".join(re.escape(tok) for tok in sorted(allowed_special,key=len,reverse=True)+ \")\"\n",
    "                             \n",
    "                ))\n",
    "            \n",
    "    \n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids,mode=\"most\"):\n",
    "        pairs=Counter(zip(token_ids,token_ids[1:]))\n",
    "\n",
    "        if not pairs:\n",
    "            return None\n",
    "        if mode==\"most\":\n",
    "            return max(pairs.items(),key=lambda x:x[1])[0]\n",
    "        if mode==\"least\":\n",
    "            return min(pairs.items(),key=lambda x:x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'most' or 'least'\")\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids,pair_id,new_id):\n",
    "        dq=deque(token_ids)\n",
    "        replaced=[]\n",
    "        while dq:\n",
    "            current=dq.popleft()\n",
    "            if dq and (current,dq[0])==pair_id:\n",
    "                replaced.append(new_id)\n",
    "                ## Remove the 2nd token of the pair 1st was already removed\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "        return replaced\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a3fbfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(100, 102): 1, (102, 104): 1, (104, 1000): 1, (1000, 1000): 2})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_id=[100,102,104,1000,1000,1000]\n",
    "pairs=Counter(zip(tokens_id,tokens_id[1:]))\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e29b33fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 1000), 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(pairs.items(),key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d00a952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_freq_pair(token_ids,mode=\"most\"):\n",
    "        pairs=Counter(zip(token_ids,token_ids[1:]))\n",
    "\n",
    "        if not pairs:\n",
    "            return None\n",
    "        if mode==\"most\":\n",
    "            return max(pairs.items(),key=lambda x:x[1])[0]\n",
    "        if mode==\"least\":\n",
    "            return min(pairs.items(),key=lambda x:x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'most' or 'least'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43dc9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_pair(token_ids, pair_id, new_id):\n",
    "    from collections import deque\n",
    "    ## list-like container with fast appends and pops on either end\n",
    "    dq = deque(token_ids)\n",
    "    replaced = []\n",
    "    while dq:\n",
    "        ## Remove and return an element from the left side of the deque. If no elements are present, raises an IndexError.\n",
    "        current = dq.popleft()\n",
    "        \n",
    "        if dq and (current, dq[0]) == pair_id:\n",
    "            print(\"replacing\",current,dq[0])\n",
    "            replaced.append(new_id)\n",
    "            dq.popleft()  # Remove second element of the pair\n",
    "        else:\n",
    "            replaced.append(current)\n",
    "    return replaced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f1c29ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([1, 2, 3, 2, 3, 4, 5])\n",
      "1\n",
      "deque([2, 3, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "dq=deque(token_ids)\n",
    "print(dq)\n",
    "print(dq.popleft())\n",
    "print(dq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b89714e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 50)\n",
      "replacing 40 50\n",
      "replacing 40 50\n",
      "replacing 40 50\n",
      "[1, 2, 70, 2, 3, 99, 99, 99]\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "token_ids = [1, 2, 70, 2, 3, 40, 50,40,50,40,50]\n",
    "pair_id = find_freq_pair(token_ids)\n",
    "print(pair_id)\n",
    "new_id = 99\n",
    "\n",
    "# Expected behavior:\n",
    "# Finds every (2, 3) pair and replaces it with 99.\n",
    "# So: [1, 2, 3, 2, 3, 4, 5] → [1, 99, 99, 4, 5]\n",
    "\n",
    "output = replace_pair(token_ids, pair_id, new_id)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fa6e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_special={\"<|endoftext|>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5eb90633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text,allowed_special=None):\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs, with tiktoken-style handling of special tokens.\n",
    "    \n",
    "        Args:\n",
    "            text (str): The input text to encode.\n",
    "            allowed_special (set or None): Special tokens to allow passthrough. If None, special handling is disabled.\n",
    "    \n",
    "        Returns:\n",
    "            List of token IDs.\n",
    "        \"\"\"\n",
    "        import re\n",
    "        token_ids=[]\n",
    "\n",
    "        ## IF SPECIAL TOKEN HANDLING IS ENABLED\n",
    "        if allowed_special is not None and len(allowed_special)>0:\n",
    "            print(allowed_special)\n",
    "            ## BUILD REGEX TO MATCH ALLOWED SPECIAL TOKENS\n",
    "            special_pattern=(\n",
    "                \"(\"+\"|\".join(re.escape(tok) for tok in sorted(allowed_special,key=len,reverse=True)+ \")\"\n",
    "                             \n",
    "                ))\n",
    "\n",
    "        last_index=0\n",
    "        token_ids=[]\n",
    "        text=\"hello how are you?\"\n",
    "        for match in  re.finditer(special_pattern,text):\n",
    "            prefix=text[last_index:match.start()]\n",
    "            token_ids.append(encode(prefix,allowed_special=None))\n",
    "            print(prefix,token_ids)\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa71d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, allowed_special=None):\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs, with tiktoken-style handling of special tokens.\n",
    "    \n",
    "        Args:\n",
    "            text (str): The input text to encode.\n",
    "            allowed_special (set or None): Special tokens to allow passthrough. If None, special handling is disabled.\n",
    "    \n",
    "        Returns:\n",
    "            List of token IDs.\n",
    "        \"\"\"\n",
    "        import re\n",
    "    \n",
    "        token_ids = []\n",
    "    \n",
    "        # If special token handling is enabled\n",
    "        if allowed_special is not None and len(allowed_special) > 0:\n",
    "            # Build regex to match allowed special tokens\n",
    "            special_pattern = (\n",
    "                \"(\" + \"|\".join(re.escape(tok) for tok in sorted(allowed_special, key=len, reverse=True)) + \")\"\n",
    "            )\n",
    "    \n",
    "            last_index = 0\n",
    "            for match in re.finditer(special_pattern, text):\n",
    "                prefix = text[last_index:match.start()]\n",
    "                token_ids.extend(encode(prefix, allowed_special=None))\n",
    "\n",
    "                special_token=match.group(0)\n",
    "                if special_token in inverse_vocab:\n",
    "                     token_ids.append(inverse_vocab[special_token])\n",
    "                else:\n",
    "                     raise ValueError(f\"Special token {special_token} not found in vocabulary.\")\n",
    "                last_index = match.end()\n",
    "            \n",
    "            text = text[last_index:]\n",
    "\n",
    "            \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "102f2ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Jack embraced beauty through art and life.<|endoftext|> \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb0cff5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43mallowed_special\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<|endoftext|>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36mencode\u001b[1;34m(text, allowed_special)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfinditer(special_pattern, text):\n\u001b[0;32m     25\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m text[last_index:match\u001b[38;5;241m.\u001b[39mstart()]\n\u001b[1;32m---> 26\u001b[0m     \u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_special\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "encode(input_text,allowed_special={'<|endoftext|>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0049bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
